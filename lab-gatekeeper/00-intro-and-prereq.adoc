image:logoopenshift.png[logoopenshift]
image:logokubernetes.png[logokubernetes]
image:logogatekeeper.svg[logogatekeeper, width=200, height=200]

= Introduction & Warm up
:author: Coral Romero
:email: cromerob@redhat.com
:imagesdir: ./images
:toc: left
:toc-title: Lab 0 - Warming up

[Abstract]

WARNING: This is not a recommended practices guide nor intended to serve as a reference. These series of labs are oriented in such a way that you will have a first contact to the main concepts of Open Policy Agent and Gatekeeper operator.

In this preparation lab, we are going to get the `oc` cli binary and validate that all necessary requirements are successfully accomplished before starting the series of labs. 

Along these labs, you will refer to a list of variables that have been shared with you before starting the labs. If this is not your case, please let us know. 

These variables are:

- *$OCP_CONSOLE*: url for accessing to the 
Openshift web console
- *$OCP_API*: Openshift API endpoint. We will use this endpoint when accesing the cluster from the `oc` cli.
- *$USER*: your private user id used for Openshift authentication
- *$PASSWORD*:  your secret password used for Openshift authentication
- *$APPS_NS*: namespace where you will deploy your apps during the whole series of labs

Feel free to export these variables in your environment.

There is also a list of files needed to run the labs. These files can be found in the following github repository:

https://gitlab.consulting.redhat.com/cromerob/lab-opa-gatekeeper/-/tree/master/lab-gatekeeper-files

Clone the repository to your desired location, that will then be referred as *$LABS_HOME* during the labs:

....
$ git clone https://gitlab.consulting.redhat.com/cromerob/lab-opa-gatekeeper.git $LABS_HOME
....

:numbered:
== Access to Openshift Web Console

Use the `*$OCP_CONSOLE*` url to acces the Openshift Web Console. 

A login screen will be shown as follow after selecting `Local Password`:

image:login.png[login]

Enter your `username` and `password` and click `Log In`.

If you have successfully logged in, you will landed at the main dashboard with your current projects. As seen below, you should have access to 4 namespaces, `openshift-operators`, `petclinic-bluegreen-$USER`, `petclinic-beta-$USER` and `openshift-gatekeeper-system`.

image:namespaces.png[namespaces]

It is time to get the Openshift CLI binaries for your specific OS. 

=== Download the CLI 

On the top-right (?) menu, click on `Command line tools`:

image:menu.png[menu]

Then, choose the binaries according to your OS:

image:binaries.png[binaries]


Extract the binaries and try it by executing the following:

....
$ oc get clusterversion

NAME      VERSION   AVAILABLE   PROGRESSING   SINCE   STATUS
version   4.7.12    True        False         5d      Cluster version is 4.7.12
....

The above is an example of what you should get when running the command.

Finally verify you can log in from your terminal:

----
oc login -u $USER -p $PASSWORD $OCP_API
Login successful.

You have access to the following projects and can switch between them with ' project <projectname>':

  * openshift-gatekeeper-system
    openshift-operators
    petclinic-beta-$USER
    petclinic-bluegreen-$USER
----

=== Gatekeeper operator


Gatekeeper operator can be installed via Red Hat Marketplace or by creating a Subscription resource. For installing an operator you need to have `cluster-admin` role while your user has `gatekeeper-ops-role`.
This role limits the privileges of your user so you won't be able to run cluster wide actions but you will have enough privileges for create, get, list, delete, patch, update and watch gatekeeper resources among others.

Furthermore you will find a `Gatekeeper` resource created with the basic configuration needed for logs and audit feature.

You can check the installed operator for this lab on `Operators` -> `Installed Operators`:

image:operator.png[operator]

Additionally you can check existing `Gatekeeper` resource with command:

WARNING: Your user only has permissions to get and list this resource.

----
oc get gatekeepers gatekeeper -o yaml
----

Otherwise you can navigate to `Gatekeeper` tab on the operator section:

image:gatekeeper.png[gatekeeper]

== Gatekeeper operator features

=== Constraint Templates

Constraint templates define the pattern of the constraint and the Rego rule. Most of these patterns are defined by iterating through the resource the constraint is auditing and comparing this value with the threshold specified by the constraint. You can use logical operator, type transformation and looping as part of it.

Here you can find information about the https://www.openpolicyagent.org/docs/latest/policy-reference/[policy reference].

=== Constraints

Constraint resources define how the template must be enforced as it specifies the value to be hooked and how. Constraints must be created after the template is implemented. 

Constraints define:
 
 - Parameters: threshold values.
 - Kinds: list of object to which the constraint will apply.
 - Scope: cluster-scoped or namespace-scope resources affected by the constraint. This works together with namespaces excluded by config file.
 - Namespace: apply the constraint to an specific namespace.
 - Excluded namespace: apply the constraint to a non listed namespace.
 - Label selector: apply constraint to these labeled resources.
 - Namespace selector: apply constraint to specific synced namespaces.

=== Admission webhook

Gatekeeper is a Kubernetes admission webhook resource which defines two different admission webhooks, one for checking a request against the installed constraints and another one for checking labels on namespace requests to bypass certain constraints.

Webhooks values like timeouts and failure policy can be configured to ignore certain type of errors, allow request in specific conditions, tune performance or customize availability. Changing these configuration is not covered on this lab but you can find the information https://open-policy-agent.github.io/gatekeeper/website/docs/customize-admission[here].

You can check current admission hook configuration with this command:

WARNING: Your user only has permissions to get and list this resource.

[source, bash]
----
oc get ValidatingWebhookConfiguration gatekeeper-validating-webhook-configuration -o yaml
----

=== Config

The `Config` resource is a resource which can be used to define general configuration for Gatekeeper. 
The two main configurations we can do are Sync and Match:

- `Sync`: replicate and sync data to OPA, so these data is available for constraints which need to access more objects that the one under test.
- `Match`: list namespaces to be excluded by their names and determine the process among these options: "audit", "webhook", "sync" and "*".


=== Audit, Syncing and Debugging

==== Audit

Audit feature register all the events related to the status of a constraint by enabling periodic evaluation of resources against the policies.
Audit configuration values like memory consumption, scope or limits can be overrided to improve performance. Those are defined as part of the Config resource previously mentioned.
Some of these values are:

- Constraint violations limit: default to 20.
- Audit chunk size: default to infinite. To limit memory consumption of the auditing Pod.
- Audit interval: default to 60 seconds. 
- Audit from cache: default to false. Audit will request each resource from the Kubernetes API during each cycle of the audit unless you specify this flad and define a match kind resource, in this case it will be audited from cache. Auditing from cache saves time as it doesn't have to audit all resources in the cluster. Not defining match kind resources is equal to set this flag to false.

==== Debugging

Constraints must specify an enforcement action which is `deny` by default. Other option is `dryrun` mode which allows to test constraint without making actual changes while are registered as violations in the audit status section.
Logs details are configured when creating the Gatekeeper resource. Log levels ranges between `DEBUG`, `INFO`, `WARNING` and `ERROR`.

Additionally in Config resource you can enable traces for some resources and a specific user. These traces will be logged to the stdout of the Gatekeeper controller.

==== Syncing

Config resource defines a list of object to be synced by defining group, version and kind. Once this list of objects is synced, they can be accesed via data inventory document following this structure:

 -  `data.inventory.cluster-group-kind-name`
 -  `data.inventory.namespace-group-kind-name`

This feature is interesting not only for its potential to improve performance but it allows to implement rules which require access to other resources than the one observed directly by the rule.

