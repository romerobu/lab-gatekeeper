= Lab 5 - Sync data
:author: Coral Romero
:email: cromerob@redhat.com
:imagesdir: ./images
:toc: left
:toc-title: Sync data


[Abstract]
In this lab we are going to implement a constraint to see how the syncing feature works. 

As explaing at the previous presentation, syncing feature allows to sync objects in order to  make them accessible apart from the resource under test.
For audit feature, every object under test is cached before being audited for constraints while syncing allows us to cache more objects and use OPA cache "as the source-of-truth". Those objects to be synced are specified at the `syncOnly` section.

Once data is replicated, it will be accessible via `data.inventory`.

For this example we want to verify that before deploying any pod there should be a deployed `ResourceQuota` on that `namespace`.


:numbered:
== Deploy constraint

Before deploying any constraint, let's verify again that `Config` resource defines namespaces to be synced.

*CLI*:

----
$ oc get config.config.gatekeeper.sh/config -o yaml -n openshift-gatekeeper-system

 ...
   sync:
    syncOnly:
    - group: ""
      kind: ResourceQuota
      version: 'v1'
 ...
----

*Web Console*:

Go to `Explore`, search for `Config`, then on the upper banner select `openshift-gatekeeper-system` namespace and select your instance.

Finally you will be able to see the yaml definition:

image:syncconfig.png[configinstance]

You will see a list of objects on the `Sync` section, those are the resources accessed via `inventory`.

=== Deploy Constraint Template

First of all we are going to check the constraint template where we are going to define how a namespace should have a deployed `ResourceQuota` for deploying any resource.

Additionally, on this constraint you can see how data inventory is accessed using the syntax we introduced at the beginning:

As this is a cluster scoped resource, constraint template has been already deployed for you so you just need to create your parameterized constraint.

*CLI*:

----
$ oc get constraintTemplate requiredresourcequota -o yaml
----

If you take a look to this template, you will see how you are accessing the data inventory and comparing this data.

----
apiVersion: templates.gatekeeper.sh/v1beta1
kind: ConstraintTemplate
metadata:
  name: resourcequota
spec:
  crd:
    spec:
      names:
        kind: ResourceQuota
      validation:
        openAPIV3Schema:
          properties:
            name:
              type: string       
  targets:
    - target: admission.k8s.gatekeeper.sh
      rego: |
        package resourcequota
        violation[{"msg": msg}] {
          requestns := input.review.object.metadata.namespace
          existingrqs := {e | e := data.inventory.namespace[requestns]["v1"]["ResourceQuota"][_]["metadata"]["namespace"]}
          not ns_exists(requestns,existingrqs)
          msg = sprintf("The Pod could not be created because the %v namespace doesn't contain any ResourceQuota object",[requestns])
        }
        ns_exists(ns,arr) {
          arr[_] = ns
        }
----

*Web Console*:

Go to `Home` -> `Explore` -> Type `ConstraintTemplate` -> Select `v1beta1` version.

Once you have selected `ConstraintTemplate` you will navigate to a page where you see the resource details.

Go to `Instances` tab and click on the existing constraint template `requiredresourcequota` to see more details:

image:instancequota.png[instancequota]

=== Deploy Constraint

Then we have to deploy constraint where we are going to define the parameters:

 - Namespace: `petclinic-bluegreen-$USER` and `petclinic-test-$USER`.
 - Resource under test: `Deployment`.
 - Enforcement action: `deny`.

*CLI*:

----
$ oc process -f lab-gatekeeper-files/lab5/constraint.yaml -p USER=$USER  | oc apply -f -

 requiredresourcequota.constraints.gatekeeper.sh/requiredresourcequota${USER} created
----

*Web Console*:

After creating the instance you should see the recently created resource in a list. Then as per your yaml definition you should be able to list a  CRD called `RequiredResourceQuota` in the main menu.

Repeat the same procedure for this new CRD and paste your yaml definition after changing the ${USER} value for your username:

WARNING: It may take a while till those are listed.

----
apiVersion: constraints.gatekeeper.sh/v1beta1
kind: RequiredResourceQuota
metadata:
  name: requiredresourcequota${USER}
spec:
  enforcementAction: deny      
  match:
    namespaces:
      - "petclinic-bluegreen-${USER}"      
      - "petclinic-test-${USER}"
    kinds:
      - apiGroups: ["*"]
        kinds: ["Deployment"]
----

== Test constraint

=== Test negative

For testing this constraint, our environment has two namespaces, one with a deployed resource quota resource `petclinic-bluegreen-$USER` and another now without it.

For testing the negative case we are going to try to deploy an app into a namespace without a resource quota:

*CLI*:

----
$ oc apply -f lab-gatekeeper-files/lab5/deployment-app-blue.yaml -n petclinic-test-$USER

  Error from server ([denied by requiredresourcequotauser3] Error:The Pod could not be created because the petclinic-test-${USER} namespace doesn't contain any ResourceQuota object): error when creating "lab-gatekeeper-files/lab5/deployment-app-green.yaml": admission webhook "validation.gatekeeper.sh" denied the request: [denied by requiredresourcequota${USER}] Error:The Pod could not be created because the petclinic-test-${USER} namespace doesn't contain any ResourceQuota object

----

*Web Console*:

As in the previous labs in namespace `petclinic-test-$USER`:

----
kind: Deployment
apiVersion: apps/v1
metadata:
  name: quarkus-petclinic-blue
  labels:
    app: quarkus-petclinic-blue
spec:
  replicas: 2
  selector:
    matchLabels:
      app: quarkus-petclinic-blue
  template:
    metadata:
      labels:
        app: quarkus-petclinic-blue
        deployment: quarkus-petclinic-blue
    spec:
      containers:
        - name: quarkus-petclinic
          image: 'quay.io/dsanchor/quarkus-petclinic:in-mem'
          ports:
            - containerPort: 8080
              protocol: TCP
          resources:
            limits:
              cpu: "200m"
              memory: "200Mi"
            requests:
              cpu: "100m"
              memory: "100Mi"
          imagePullPolicy: Always
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxUnavailable: 25%
      maxSurge: 25%
  revisionHistoryLimit: 10
  progressDeadlineSeconds: 600
----

Then go to `Explore`, search for `RequiredResourceQuota` and select your instance:

image:errorquota.png[errorquota]

As there is no resource quota, you should see error message `The Pod could not be created because the petclinic-test-$USER namespace doesn't contain any ResourceQuota object`.


=== Test positive

For testing the positive case we are going to deploy an app into a namespace with a `ResourceQuota`. As this deployment follows the existing constraint there shouldn't be any issue.

*CLI*:

----
$ oc apply -f lab-gatekeeper-files/lab5/deployment-app-green.yaml -n petclinic-bluegreen-$USER

 deployment.apps/quarkus-petclinic-green created
 route.route.openshift.io/route-petclinic-bluegreen created
 service/quarkus-petclinic-service created

$ oc apply -f lab-gatekeeper-files/lab5/deployment-app-blue.yaml -n petclinic-bluegreen-$USER

 deployment.apps/quarkus-petclinic-blue created
 service/quarkus-petclinic-blue created
----

*Web Console*:

Now try to redeploy you app in the namespace `petclinic-bluegreen-${USER}` with a deployed `ResourceQuota`.

----
kind: Deployment
apiVersion: apps/v1
metadata:
  name: quarkus-petclinic-green
  labels:
    app: quarkus-petclinic-green
spec:
  replicas: 2
  selector:
    matchLabels:
      app: quarkus-petclinic-green
  template:
    metadata:
      labels:
        app: quarkus-petclinic-green
        deployment: quarkus-petclinic-green
    spec:
      containers:
        - name: quarkus-petclinic
          image: 'quay.io/dsanchor/quarkus-petclinic:in-mem'
          ports:
            - containerPort: 8080
              protocol: TCP
          resources:
            limits:
              cpu: "200m"
              memory: "200Mi"
            requests:
              cpu: "100m"
              memory: "100Mi"
          imagePullPolicy: Always
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxUnavailable: 25%
      maxSurge: 25%
  revisionHistoryLimit: 10
  progressDeadlineSeconds: 600
----

----
kind: Route
apiVersion: route.openshift.io/v1
metadata:
  name: route-petclinic-bluegreen
  labels:
    app: quarkus-petclinic-green
spec:
  to:
    kind: Service
    name: quarkus-petclinic-green
    weight: 100
  port:
     targetPort: 8080-tcp
  wildcardPolicy: None
----

----
kind: Service
apiVersion: v1
metadata:
  name: quarkus-petclinic-green
  labels:
    app: quarkus-petclinic-green
spec:
  ports:
    - name: 8080-tcp
      protocol: TCP
      port: 8080
      targetPort: 8080
  selector:
    app: quarkus-petclinic-green
    deployment: quarkus-petclinic-green
  type: ClusterIP
  sessionAffinity: None
----

----
kind: Deployment
apiVersion: apps/v1
metadata:
  name: quarkus-petclinic-blue
  labels:
    app: quarkus-petclinic-blue
spec:
  replicas: 2
  selector:
    matchLabels:
      app: quarkus-petclinic-blue
  template:
    metadata:
      labels:
        app: quarkus-petclinic-blue
        deployment: quarkus-petclinic-blue
    spec:
      containers:
        - name: quarkus-petclinic
          image: 'quay.io/dsanchor/quarkus-petclinic:in-mem'
          ports:
            - containerPort: 8080
              protocol: TCP
          imagePullPolicy: Always
          resources:
            limits:
              cpu: "200m"
              memory: "200Mi"
            requests:
              cpu: "100m"
              memory: "100Mi"
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxUnavailable: 25%
      maxSurge: 25%
  revisionHistoryLimit: 10
  progressDeadlineSeconds: 600
----

----
kind: Service
apiVersion: v1
metadata:
  name: quarkus-petclinic-blue
  labels:
    app: quarkus-petclinic-blue
spec:
  ports:
    - name: 8080-tcp
      protocol: TCP
      port: 8080
      targetPort: 8080
  selector:
    app: quarkus-petclinic-blue
    deployment: quarkus-petclinic-blue
  type: ClusterIP
  sessionAffinity: None
----

After finishing this lab, you should have two apps deployed (blue and green) on namespace petclinic-bluegreen-${USER} which accomplish every constraint deployed.

You can try to switch between green and blue deployment and app should be running and reachable:

----
$ oc patch route/bluegreen -p '{"spec":{"to":{"name":"quarkus-petclinic-blue"}}}'

  route.route.openshift.io/route-petclinic-bluegreen patched
----

----
$ oc get route -o jsonpath='{range .items[*].spec}{"Host: "}{.host}{"\n"}{end}' -n petclinic-bluegreen-$USER

  Host: quarkus-petclinic-bluegreen-petclinic-bluegreen-${USER}.apps.${APPS_CLUSTER}.opentlc.com
----

This means you managed to deploy a functional blue/green deployment application which accomplish these restrictions:

- Maximun replicas allowed
- Required labels for deployment, service and route
- Required route valid pattern route name
- Required resource quota per namespace
- Restricted pod ratio consumption

This is just an example of what you can do by implementing Open Policy Agent and Gatekeeper operator.
